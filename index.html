<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
    <title>Lijie Fan's Homepage</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <!--[if lte IE 8]>
    <script src="assets/js/ie/html5shiv.js"></script><![endif]-->
    <link rel="stylesheet" href="assets/css/main.css"/>
    <!--[if lte IE 8]>
    <link rel="stylesheet" href="assets/css/ie8.css"/><![endif]-->
    <!-- statistics -->
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?c65695d70e5decd45683cf0d863f1d56";
            var s = document.getElementsByTagName("script")[0];
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

</head>
<body>

<!-- Header -->
<section id="header">
    <header>
        <span class="image avatar"><img src="images/photo_new.jpg" alt="" align="middle"></span>
        <h1 id="logo"><a href="#" class="big_name">Lijie Fan</a></h1>
        <!-- <p>A Deep Learner.</p> -->
    </header>
    <nav id="nav">
        <ul>
            <li><a href="#1" class="active">About</a></li>
            <li><a href="#2">Publications</a></li>
            <li><a href="#3">Professional Services</a></li>
            <li><a href="#4">Misc</a></li>
        </ul>
    </nav>
</section>

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">

        <!-- One -->
        <section id="1">
            <div class="container" style="text-align: justify;">
                <header class="major">
                    <h2>Lijie Fan </h2>
                </header>
                <p>
                    <a href="https://scholar.google.com/citations?user=qthDk3oAAAAJ&hl=en&oi=ao"><font color="#222"><i
                            class="fa fa-graduation-cap fa-2x" width="24px" height="24px"
                            style="position:relative;top: -6px; left:2px"></i></font></a> &nbsp
                    <a href="https://www.linkedin.com/in/lijie-fan-06a122148/"><font color="#222"><i
                            class="fa fa-linkedin fa-2x" width="24px" height="24px"
                            style="position:relative;top: -6px; left:2px"></i></font></a> &nbsp
                    <a href="https://github.com/LijieFan"><font color="#222"><i class="fa fa-github fa-2x" width="24px"
                                                                                height="24px"
                                                                                style="position:relative;top: -6px; left:2px"></i>
                    </font></a> &nbsp
                    <a href="https://twitter.com/lijie_fan"><font color="#222"><i class="fa fa-twitter fa-2x"
                                                                                  width="24px" height="24px"
                                                                                  style="position:relative;top: -6px; left:2px"></i>
                    </font></a> &nbsp
                </p>
                <div style="margin-bottom: 25px; margin-top: -30px;">
                    I am a research scientist at Google DeepMind.
                    <br><br>
                    I completed my PhD in Computer Science at <font color="#4183C4"><a href="https://www.eecs.mit.edu">MIT EECS</a></font>,
                    where I also got my master's degree.
                    Before that I obtained my bachelorâ€™s degree in Computer Science from <font color="#4183C4"><a
                        href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a></font>.
                    <br><br>
                    I train large scale autoregressive models that generate multimodal outputs. I am the tech lead / core contributor to Fluid, UniFluid, and Gemini Multimodal Generation.
                </div>
                <b>Email</b>: lijiefan[at]alum.mit.edu
                <br><br>
            </div>
        </section>

        <section id="2">
            <div class="container">
                <h3 style="padding-top: 0; margin-top: 0; margin-bottom: 0;">Publications
<!--                    <font size="4em" style="font-weight: normal;">(-->
<!--                        <font color="#4183C4"><a href="javascript:;" onclick="showSelected();">show selected</a></font>-->
<!--                        /-->
<!--                        <font color="#4183C4"><a href="javascript:;" onclick="showAll();">show all by date</a></font>-->
<!--                        )</font>-->
                </h3>
                <div style="margin-top:-0.3em; margin-bottom: 1.2em">*: equal contribution</div>
                <div>
                <div class="features" data-selected="true">
                    <article>
                        <a href="images/gemini.png" class="image"><img src="images/gemini.png" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</a></font></b><br>
                            Google Gemini Team<br>
                            <i> <b><font color="#000000">Tech Report</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf">Paper</a></font>
                        </div>
                    </article>
                    <article>
                        <a href="images/unifluid.jpg" class="image"><img src="images/unifluid.jpg" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Unified Autoregressive Visual Generation and Understanding with Continuous Tokens</a></font></b><br>
                            <b><font color="#000000">Lijie Fan*</font></b>, Luming Tang*, Siyang Qin*, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut<br>
                            <i> <b><font color="#000000">Preprint</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2503.13436">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2503.13436"">arXiv</a></font>
                        </div>
                    </article>
                </div>
                <div class="features" data-selected="true">
                    <article>
                        <a href="images/fluid.jpg" class="image"><img src="images/fluid.jpg" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</a></font></b><br>
                            <b><font color="#000000">Lijie Fan*</font></b>, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian*<br>
                            <i> <b><font color="#000000">ICLR 2025</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2410.13863">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2410.13863">arXiv</a></font>
                        </div>
                    </article>
                </div>
                <div class="features" data-selected="true">
                    <article>
                        <a href="images/fractalgen.gif" class="image"><img src="images/fractalgen.gif" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Fractal Generative Models</a></font></b><br>
                            Tianhong Li, Qinyi Sun <b><font color="#000000">Lijie Fan</font></b>,
                            Kaiming He<br>
                            <i> <b><font color="#000000">Preprint</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2502.17437.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2502.17437">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/LTH14/fractalgen">code</a></font>
                        </div>
                    </article>
                </div>
                <div class="features" data-selected="true">
                    <article>
                        <a href="images/synclr.png" class="image"><img src="images/synclr.png" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Learning Vision from Models Rivals Learning Vision from Data</a></font></b><br>
                            Yonglong Tian*, <b><font color="#000000">Lijie Fan*</font></b>,
                            Kaifeng Chen, Dina Katabi, Dilip Krishnan, Phillip Isola<br>
                            <i> <b><font color="#000000">CVPR 2024</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2312.17742.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2312.17742">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/google-research/syn-rep-learn">code</a></font>
                        </div>
                    </article>
                </div>
                <div class="features" data-selected="true">
                    <article>
                        <a href="images/scaling.png" class="image"><img src="images/scaling.png" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Scaling Laws of Synthetic Images for Model Training ... for
                                Now</a></font></b><br>
                            <b><font color="#000000">Lijie Fan*</font></b>,
                            Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian*<br>
                            <i> <b><font color="#000000">CVPR 2024</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2312.04567.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2312.04567">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/google-research/syn-rep-learn">code</a></font>
                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/laclip.png" class="image"><img src="images/laclip.png" alt=""
                                                                       !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Improving CLIP Training with Language Rewrites</a></font></b><br>
                            <b><font color="#000000">Lijie Fan*</font></b>, Dilip Krishnan, Phillip Isola, Dina Katabi,
                            Yonglong Tian*
                            <br>
                            <i> <b><font color="#000000">NeurIPS 2023</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2305.20088.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2305.20088">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/LijieFan/LaCLIP">code</a></font>
                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/stablerep.png" class="image"><img src="images/stablerep.png" alt=""
                                                                          !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">StableRep: Synthetic Images from Text-to-Image Models Make Strong
                                Visual Representation Learners</a></font></b><br>
                            Yonglong Tian*, <b><font color="#000000">Lijie Fan*</font></b>,
                            Phillip Isola, Huiwen Chang, Dilip Krishnan<br>
                            <i> <b><font color="#000000">NeurIPS 2023</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2306.00984.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2306.00984">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/google-research/syn-rep-learn">code</a></font> /  </a></font>
                            <font
                                    color="#4183C4"><a
                                    href="https://news.mit.edu/2023/synthetic-imagery-sets-new-bar-ai-training-efficiency-1120">MIT
                                News</a></font>
                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/reparo.png" class="image"><img src="images/reparo.png" alt=""
                                                                       !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Reparo: Loss-Resilient Generative Codec for Video
                                Conferencing</a></font></b><br>
                            Tianhong Li, Vibhaalakshmi Sivaraman, <b><font color="#000000">Lijie Fan</font></b>,
                            Mohammad Alizadeh, Dina Katabi<br>
                            <i> <b><font color="#000000">Preprint</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://arxiv.org/pdf/2305.14135.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2305.14135">arXiv</a></font>
                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/DependencyViT.png" class="image"><img src="images/DependencyViT.png" alt=""
                                                                              !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Visual Dependency Transformers: Dependency Tree Emerges from
                                Reversed Attention</a></font></b><br>
                            Mingyu Ding, Yikang Shen, <b><font color="#000000">Lijie Fan</font></b>, Zhenfang Chen,
                            Zitian Chen, Ping Luo, Joshua B Tenenbaum, Chuang Gan<br>
                            <i> <b><font color="#000000">CVPR 2023</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Visual_Dependency_Transformers_Dependency_Tree_Emerges_From_Reversed_Attention_CVPR_2023_paper.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2304.03282">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/dingmyu/DependencyViT">code</a></font>

                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/rcl.png" class="image"><img src="images/rcl.png" alt=""
                                                                    !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Making Contrastive Learning Robust to Shortcuts</a></font></b><br>
                            Tianhong Li*, <b><font color="#000000">Lijie Fan*</font></b>, Yuan Yuan, Hao He, Yonglong
                            Tian, Rogerio Feris, Piotr Indyk, Dina Katabi<br>
                            <i> <b><font color="#000000">WACV 2023</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://openaccess.thecvf.com/content/WACV2023/papers/Li_Addressing_Feature_Suppression_in_Unsupervised_Visual_Representations_WACV_2023_paper.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2012.09962">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://www.youtube.com/watch?v=wbtAOIS16LY">Talk (by Dina)</a></font>

                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/tsc.png" class="image"><img src="images/tsc.png" alt=""
                                                                    !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Targeted supervised contrastive learning for long-tailed
                                recognition</a></font></b><br>
                            Tianhong Li*, Peng Cao*, Yuan Yuan, <b><font color="#000000">Lijie Fan</font></b>, Yuzhe
                            Yang, Rogerio Feris, Piotr Indyk, Dina Katabi<br>
                            <i> <b><font color="#000000">CVPR 2022</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2111.13998">arXiv</a></font> /  </a></font> <font
                                color="#4183C4"><a
                                href="https://github.com/LTH14/targeted-supcon">code</a></font>

                        </div>
                    </article>
                </div>
                <div class="features">
                    <article>
                        <a href="images/rfunsup.png" class="image"><img src="images/rfunsup.png" alt=""
                                                                        !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Unsupervised Learning for Human Sensing Using Radio
                                Signals</a></font></b><br>
                            Tianhong Li*, <b><font color="#000000">Lijie Fan*</font></b>, Yuan Yuan*, Dina Katabi<br>
                            <i> <b><font color="#000000">WACV 2022</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="https://openaccess.thecvf.com/content/WACV2022/papers/Li_Unsupervised_Learning_for_Human_Sensing_Using_Radio_Signals_WACV_2022_paper.pdf">PDF</a></font>
                            / <font color="#4183C4"><a
                                href="https://arxiv.org/abs/2207.02370">arXiv</a></font>
                        </div>
                    </article>
                    <article>
                        <a href="images/advcl.png" class="image"><img src="images/advcl.png" alt=""
                                                                      !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">When Does Contrastive Learning Preserve Adversarial Robustness from
                                Pretraining to Finetuning?</a></font></b><br>
                            <b><font color="#000000">Lijie Fan</font></b>, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, Chuang
                            Gan<br>
                            <i> <b><font color="#000000">NeurIPS 2021</font></b></i>&nbsp;&nbsp;
                            <font color="#4183C4"><a href="project_webpage/AdvCL_Neurips/index.html" class="hove">Project
                                Page</a></font> / <font color="#4183C4"><a
                                href="project_webpage/AdvCL_Neurips/papers/AdvCL_Neurips.pdf">PDF</a></font> / <font
                                color="#4183C4"><a href="https://arxiv.org/abs/2111.01124">arXiv</a></font>
                            / <font color="#4183C4"><a href="https://github.com/LijieFan/AdvCL"> Code</a></font>
                            / <font color="#4183C4"><a
                                href="https://bdtechtalks.com/2021/11/18/contrastive-learning-adversarial-attacks/">TechTalks</a></font>

                        </div>
                    </article>
                    <article>
                        <a href="images/eccv_img.png" class="image"><img src="images/eccv_img.png" alt=""
                                                                         !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">In-Home Daily-Life Captioning Using Radio
                                Signals</a></font></b><br>
                            <b><font color="#000000">Lijie Fan*</font></b>, Tianhong Li*, Yuan Yuan, Dina Katabi<br>
                            <i> <b><font color="#000000">ECCV 2020</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="http://rf-diary.csail.mit.edu/" class="hove">Project Page</a></font> / <font
                                color="#4183C4"><a href="http://rf-diary.csail.mit.edu/papers/rfdiary_eccv.pdf">PDF</a></font>
                            / <font color="#4183C4"><a href="https://arxiv.org/abs/2008.10966">arXiv</a></font> / <font
                                color="#4183C4"><a href="http://rf-diary.csail.mit.edu/slides/longtalk.pdf"> Slides </a></font>
                            / <font color="#4183C4"><a href="https://youtu.be/j528nQs4_a8"> Demo </a></font> / <font
                                color="#4183C4"><a href="https://youtu.be/LA-JW4_ovAQ"> Video </a></font> / <font
                                color="#4183C4"><a href="https://youtu.be/S2Y-zPJnl9U"> Talk </a></font> / <font
                                color="#4183C4"><a
                                href="https://www.csail.mit.edu/news/device-nursing-homes-can-monitor-residents-activities-permission-and-without-video">
                            CSAIL News</a></font> / <font color="#4183C4"><a
                                href="http://mms.tveyes.com/MediaCenterPlayer.aspx?u=aHR0cDovL21lZGlhY2VudGVyLnR2ZXllcy5jb20vZG93bmxvYWRnYXRld2F5LmFzcHg/VXNlcklEPTMwMzQ3OCZNRElEPTEzNzExMTM5Jk1EU2VlZD0yNTU0JlR5cGU9TWVkaWE%3D">
                            BBC</a></font> / <font color="#4183C4"><a
                                href="https://techcrunch.com/2020/08/24/mit-wireless-system-can-monitor-what-care-facility-residents-are-doing-while-preserving-privacy/">
                            TechCrunch</a></font> / <font color="#4183C4"><a
                                href="https://www.engadget.com/mit-wireless-signals-monitoring-machine-learning-rf-diary-040049578.html">
                            Engadget</a></font> / <font color="#4183C4"><a
                                href="https://venturebeat.com/2020/08/24/mit-csails-rf-diary-monitors-people-through-walls-and-in-total-darkness/">
                            VentureBeat</a></font>
                            <br><b><font color="#881900">Oral Presentation</font></b>

                        </div>
                    </article>
                    <article>
                        <a href="images/rfreid.png" class="image"><img src="images/rfreid.png" alt=""
                                                                       !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Learning Longterm Representations for Person Re-Identification
                                Using Radio Signals</a></font></b><br>
                            <b><font color="#000000">Lijie Fan*</font></b>, Tianhong Li*, Rongyao Fang*, Rumen Hristov,
                            Yuan Yuan, Dina Katabi<br>
                            <i> <b><font color="#000000">CVPR 2020</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="http://rf-reid.csail.mit.edu/" class="hove">Project Page</a></font> / <font
                                color="#4183C4"><a
                                href="http://rf-reid.csail.mit.edu/papers/rfreid_cvpr.pdf">PDF</a></font> / <font
                                color="#4183C4"><a href="https://arxiv.org/abs/2004.01091">arXiv</a></font> / <font
                                color="#4183C4"><a href="https://youtu.be/oYv30obQ8P4"> Video</a></font> / <font
                                color="#4183C4"><a
                                href="https://www.csail.mit.edu/news/home-health-device-uses-wireless-signals-identify-person-its-seen">
                            CSAIL News</a></font>
                            / <font color="#4183C4"><a
                                href="https://techcrunch.com/2020/06/16/mits-new-way-to-remotely-monitor-vital-signs-over-time-could-help-with-early-covid-19-detection-in-care-homes/">
                            TechCrunch</a></font>
                            / <font color="#4183C4"><a
                                href="https://www.yahoo.com/lifestyle/mits-way-remotely-monitor-vital-132517815.html">
                            Yahoo News</a></font>
                        </div>
                    </article>
                    <article>
                        <a href="images/iccv_img.png" class="image"><img src="images/iccv_img.png" alt=""
                                                                         !style="width:196;height:196px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Making the Invisible Visible: Action Recognition Through Walls and
                                Occlusions</a></font></b><br>
                            Tianhong Li*, <b><font color="#000000">Lijie Fan*</font></b>, Mingmin Zhao, Yingcheng Liu,
                            Dina Katabi<br>
                            <div style="padding-bottom: 10px" ;>
                                <i> <b><font color="#000000">ICCV 2019</font></b></i>&nbsp;&nbsp;<font
                                    color="#4183C4"><a href="http://rf-action.csail.mit.edu/" class="hove">Project
                                Page</a></font> / <font color="#4183C4"><a
                                    href="http://rf-action.csail.mit.edu/papers/rfaction_iccv.pdf">PDF</a></font> /
                                <font color="#4183C4"><a href="https://arxiv.org/abs/1909.09300">arXiv</a></font> /
                                <font color="#4183C4"><a href="https://youtu.be/UzjBi3xjWR4">Video</a></font> / <font
                                    color="#4183C4"><a
                                    href="https://www.technologyreview.com/2019/10/09/132696/machine-vision-has-learned-to-use-radio-waves-to-see-through-walls-and-in-darkness/">MIT
                                Technology Review</a></font>
                            </div>
                            <b><font color="#000000">Real-time Through-wall Human Activity Recognition using Radio
                                Signals</a></font></b><br>
                            <i> <b><font color="#000000">ECCV 2020 Demo</font></b></i><a
                                href="http://rf-action.csail.mit.edu/demo/index.html" class="hove">&nbsp;&nbsp;<font
                                color="#4183C4">Project Page</font></a> / <font color="#4183C4"><a
                                href="http://rf-action.csail.mit.edu/videos/demo.mp4">Video</a></font>

                        </div>
                    </article>
                    <article>
                        <a href="images/im2vid.png" class="image"><img src="images/im2vid.png" alt=""
                                                                       !style="width:196;height:256px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Controllable Image-to-Video Translation: A Case Study on Facial
                                Expression Generation</font></b>
                            <br><b><font color="#000000">Lijie Fan</font></b>, Wenbing Huang, Chuang Gan, Junzhou Huang,
                            Boqing Gong<br>
                            <i> <b><font color="#000000">AAAI 2019</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="project_webpage/image2video_arxiv/index.html" class="hove">Project Page</a></font>/<font
                                color="#4183C4"><a
                                href="project_webpage/image2video_arxiv/papers/image2video_arxiv.pdf"> PDF</a></font> /
                            <font color="#4183C4"><a href="https://arxiv.org/abs/1808.02992">arXiv</a></font>
                            <br><b><font color="#881900">Oral Presentation</font></b>

                        </div>
                    </article>
                    <article>
                        <a href="images/tvnet.png" class="image"><img src="images/tvnet.png" alt=""
                                                                      !style="!width:196;height:256px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">End-to-End Learning of Motion Representation for Video
                                Understanding</font></b>
                            <br><b><font color="#000000">Lijie Fan*</font></b>, Wenbing Huang*, Chuang Gan, Stefano
                            Ermon, Boqing Gong, Junzhou Huang<br>
                            <i> <b><font color="#000000">CVPR 2018</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="project_webpage/TVNet_cvpr/index.html" class="hove">Project Page</a></font>/<font
                                color="#4183C4"><a href="project_webpage/TVNet_cvpr/papers/TVNet_cvpr.pdf">
                            PDF</a></font> / <font color="#4183C4"><a href="https://arxiv.org/abs/1804.00413">arXiv</a></font>
                            / <font color="#4183C4"><a href="https://github.com/LijieFan/tvnet"> Code</a></font> / <font
                                color="#4183C4"><a href="https://youtu.be/GBo4sFNzhtU?t=4984">Talk</a></font>
                            <br><b><font color="#881900">Spotlight Presentation</font></b>
                        </div>
                    </article>
                    <article>
                        <a href="images/PBNets.png" class="image"><img src="images/PBNets.png" alt=""
                                                                       !style="width:196;height:256px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Towards Efficient Action Recognition: Principal Backpropagation for
                                Training Two-Stream Networks</font></b>
                            <br>Wenbing Huang*, <b><font color="#000000">Lijie Fan* </font></b>,Mehrtash Harandi, Lin
                            Ma, Huaping Liu, Wei Liu, Chuang Gan<br>
                            <i><font color="#000000">IEEE Transactions on Image Processing <b><font color="#000000">(T-IP)
                                2019</font></b></font></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="files/PBNet-TIP.pdf"> PDF</a></font>
                            </p>
                        </div>
                    </article>
                    <article>
                        <a href="images/aln.png" class="image"><img src="images/aln.png" alt=""
                                                                    !style="width:196;height:256px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Adversarial Localization Network</font></b>
                            <br><b><font color="#000000">Lijie Fan</font></b>, Shengjia Zhao, Stefano Ermon
                            <br>
                            <i><b><font color="#000000">NIPS 2017 Workshop </font></b><font color="#000000">on Learning
                                with Limited Labeled Data </font></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="files/ALN-nips17-LLD.pdf"> PDF</a></font>
                            </p>
                        </div>
                    </article>
                    <article>
                        <a href="images/efficient.png" class="image"><img src="images/efficient.png" alt=""
                                                                          !style="width:196;height:256px;"></a>
                        <div class="inner" class="inner2">
                            <b><font color="#000000">Efficient Optimization for Linear Dynamical Systems with
                                Applications to Clustering and Sparse Coding</font></b>
                            <br>Wenbing Huang, Mehrtash Harandi, Tong Zhang, <b><font color="#000000">Lijie
                            Fan</font></b>, Fuchun Sun, Junzhou Huang

                            <br>
                            <i><b><font color="#000000">NIPS 2017</font></b></i>&nbsp;&nbsp;<font color="#4183C4"><a
                                href="files/Efficient-nips17.pdf"> PDF</a></font> / <font color="#4183C4"><a
                                href="https://github.com/huangwb/LDS-toolbox"> Code</a></font>
                            </p>
                        </div>
                    </article>
                </div>
                </div>
                <section id="3">
                    <div class="container">
                        <h3>Professional Services</h3>
                        <li>Area Chair & Tutorial Chair: ICCV 2025
                        </li>
                        <li>Conference Reviewer: NeurIPS, ICML, CVPR, ICCV, ECCV, AAAI, WACV
                        </li>
                        <li>Journal Reviewer: TPAMI
                        </li>
                        <br>
                    </div>
                </section>
                <section id="4">
                    <div class="container">
                        <h3>Misc</h3>
                        <li>I do landscape photography, checkout my photos on <font color="#4183C4"><a
                                href="https://www.instagram.com/lijie_fan/">Instgram.</a></font></li>
                        <li>I'm fond of snowboarding, especially carving and ground trick.</li>
                        <br>
                    </div>
                </section>


            </div>
        </section>
    </div>
    <!-- Scripts -->
    <script type="text/javascript" id="clustrmaps"
            src="//cdn.clustrmaps.com/map_v2.js?d=u_SqqXDpBsoXaCH4_D7ksK_D3yyzqQIIJ0MOJTNXro8&cl=ffffff&w=a"></script>
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollzer.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <!--[if lte IE 8]>
    <script src="assets/js/ie/respond.min.js"></script><![endif]-->
    <script src="assets/js/main.js"></script>
<!--    <script>-->
<!--        function showAll() {-->
<!--            $("#publications").html("");-->
<!--            for (var k = 0; k < publications.length; k++) {-->
<!--                var publication = $(publications[k]);-->
<!--                $("#publications").append(publication);-->
<!--            }-->
<!--        }-->

<!--        function showSelected() {-->
<!--            $("#publications").html("");-->
<!--            for (var k = 0; k < publications.length; k++) {-->
<!--                var publication = $(publications[k]);-->
<!--                if (publication.data("selected") == true) {-->
<!--                    $("#publications").append(publication);-->
<!--                }-->
<!--            }-->
<!--        }-->

<!--        publications = $("#publications .publication");-->
<!--        showSelected();-->
<!--    </script>-->

</div>
</body>
</html>
